{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "Ponwalai Chalermwattanatrai 65340500042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up Cart-Pole Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.RL Base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**discretize_state**\n",
    "- This function takes a continuous-valued state (obs) and maps it to a discrete state.\n",
    "- The discretization is done based on discretize_state_weight, which defines the number of discrete bins for each state variable.\n",
    "- If a value is out of bounds, it is mapped to the nearest boundary.\n",
    "\n",
    "state bound\n",
    "- cart position: [-3, 3]\n",
    "- pole position: [-24 deg, 24 deg]\n",
    "- cart velocity: [-5, 5]\n",
    "- pole velocity: [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(self, obs: dict):\n",
    "    \"\"\"\n",
    "    Discretize the observation state.\n",
    "\n",
    "    Args:\n",
    "        obs (dict): Observation dictionary containing policy states.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]: Discretized state.\n",
    "    \"\"\"\n",
    "\n",
    "    # ========= put your code here =========#\n",
    "    policy_tensor = obs['policy']  # {'policy': tensor([[-0.2300,  0.0700, -0.1187, -0.1686]], device='cuda:0')}\n",
    "\n",
    "    # Extracting the four values\n",
    "    value = [0,0,0,0]\n",
    "    value[0] = policy_tensor[0, 0].item()  # First value\n",
    "    value[1] = policy_tensor[0, 1].item()  # Second value\n",
    "    value[2] = policy_tensor[0, 2].item()   # Third value\n",
    "    value[3] = policy_tensor[0, 3].item()   # Fourth value\n",
    "    discrete_value = [0,0,0,0]\n",
    "\n",
    "    bound = [[-3, 3],\n",
    "             [-np.deg2rad(24), np.deg2rad(24)],\n",
    "             [-5, 5],\n",
    "             [-5, 5]]\n",
    "\n",
    "    for i in range(0,4):\n",
    "        value[i] = max(bound[i][0], min(value[i], bound[i][1]))\n",
    "        # Compute discrete bin\n",
    "        discrete_value[i] = round((value[i] - bound[i][0]) * (self.discretize_state_weight[i] - 1) / (bound[i][1] - bound[i][0])) + 1\n",
    "\n",
    "    return (discrete_value[0],\n",
    "            discrete_value[1],\n",
    "            discrete_value[2],\n",
    "            discrete_value[3])\n",
    "    # ======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_discretize_action**\n",
    "- This function selects an action based on an epsilon-greedy policy, if unifrom random < epsilon random action else pick action from action that give max expected q value in that state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discretize_action(self, obs_dis) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            obs_dis (tuple): Discretized observation.\n",
    "\n",
    "        Returns:\n",
    "            int: Chosen discrete action index.\n",
    "        \"\"\"\n",
    "        # ========= put your code here =========#\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_of_action)\n",
    "        elif self.control_type == ControlType.DOUBLE_Q_LEARNING:\n",
    "            return int(np.argmax(self.qa_values[obs_dis]+self.qb_values[obs_dis]))\n",
    "        return int(np.argmax(self.q_values[obs_dis]))\n",
    "        # ======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapping_action**\n",
    "- map action from range [0, num_of_action] to action_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_action(self, action):\n",
    "        \"\"\"\n",
    "        Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "        Args:\n",
    "            action (int): Discrete action in range [0, n]\n",
    "            n (int): Number of discrete actions\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Scaled action tensor.\n",
    "        \"\"\"\n",
    "        # ========= put your code here =========#\n",
    "        min_action, max_action = self.action_range\n",
    "        action_step = (max_action - min_action) / (self.num_of_action - 1)\n",
    "        action_value = min_action + action * action_step\n",
    "\n",
    "        return torch.tensor([[action_value]], dtype=torch.float32)\n",
    "        # ======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**decay_epsilon**\n",
    "- decay epsilon value with exponential trend $epsilon = start epsilon * epsilon decay^{current episode}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay epsilon value to reduce exploration over time.\n",
    "        \"\"\"\n",
    "        # ========= put your code here =========#\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        return self.epsilon\n",
    "        # ======================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo**\n",
    "\n",
    "This algorithm updates Q-values using all state-action pairs from an episode, storing data in the following variables:\n",
    "- obs_hist : list of discretize state\n",
    "- action_hist : list of action index\n",
    "- reward_hist : list of reward value\n",
    "\n",
    "Update equation is\n",
    "- update cumulative reward: $G_t​=R_t​+{\\gamma}G_{t+1​}$\n",
    "    - $R_t$ is the reward at time step $t$.\n",
    "    - ${\\gamma}$ (discount factor) determines the important of future rewards.\n",
    "    - $G$ is computed backward from the last step of the episode.\n",
    "\n",
    "- update Q-values: $Q(s,a)=\\frac{Q(s,a)⋅N(s,a)+G}{N(s,a)+1}$\n",
    "    - $Q(s,a)$ is the Q-value for state-action pair $(s,a)$.\n",
    "    - $N(s,a)$ is the visit count of $(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self):\n",
    "    \"\"\"\n",
    "    Update Q-values using Monte Carlo.\n",
    "\n",
    "    This method applies the Monte Carlo update rule to improve policy decisions by updating the Q-table.\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    for t in reversed(range(len(self.reward_hist))):\n",
    "        state = self.obs_hist[t]\n",
    "        action = int(self.action_hist[t])\n",
    "        G = self.reward_hist[t] + self.discount_factor * G\n",
    "\n",
    "        # Check if the state-action pair is first visit\n",
    "        if state not in self.obs_hist[:t]:\n",
    "            self.q_values[state][action] = ((self.q_values[state][action] * (self.n_values[state][action])) + G) / (self.n_values[state][action] + 1)\n",
    "            self.n_values[state][action] += 1\n",
    "    self.obs_hist = []\n",
    "    self.action_hist = []\n",
    "    self.reward_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SARSA**\n",
    "\n",
    "This algorithm updates Q-values based on the action chosen by the current policy.\n",
    "\n",
    "Update equation is:\n",
    "- update Q-values: $Q(s,a)=Q(s,a)+{\\alpha}[R+{\\gamma}Q(s',a')−Q(s,a)]$\n",
    "    - $Q(s,a)$ is the Q-value for the current state-action pair.\n",
    "    - ${\\alpha}$ (learning rate) controls how much new information overrides old information.\n",
    "    - $R$ is the reward received after taking action $a$.\n",
    "    - ${\\gamma}$ (discount factor) determines the importance of future rewards.\n",
    "    - $Q(s',a')$ is the Q-value of the next state-action pair (following the policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state, next_action):\n",
    "    \"\"\"\n",
    "    Update Q-values using SARSA .\n",
    "\n",
    "    This method applies the SARSA update rule to improve policy decisions by updating the Q-table.\n",
    "    \"\"\"\n",
    "    q_value = self.q_values[state][action]\n",
    "    next_q_value = self.q_values[next_state][next_action] if next_state is not None else 0\n",
    "    self.q_values[state][action] = q_value + self.lr*(reward + (self.discount_factor * next_q_value) - q_value)\n",
    "    self.n_values[state][action] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q learning**\n",
    "\n",
    "This algorithm updates Q-values based on the max Q-values in next state.\n",
    "\n",
    "Update equation is:\n",
    "- update Q-values: $Q(s,a)=Q(s,a)+{\\alpha}[R+{\\gamma}\\underset{a'}{\\max}Q(s',a')−Q(s,a)]$\n",
    "    - $Q(s,a)$ is the Q-value for the current state-action pair.\n",
    "    - ${\\alpha}$ (learning rate) controls how much new information overrides old information.\n",
    "    - $R$ is the reward received after taking action $a$.\n",
    "    - ${\\gamma}$ (discount factor) determines the importance of future rewards.\n",
    "    - $\\underset{a'}{\\max}Q(s',a')$ is the maximum Q-value in the next state (off policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update Q-values using Q-Learning.\n",
    "\n",
    "        This method applies the Q-Learning update rule to improve policy decisions by updating the Q-table.\n",
    "        \"\"\"\n",
    "        q_value = self.q_values[state][action]\n",
    "        max_next_q_value = np.max(self.q_values[next_state]) if next_state is not None else 0\n",
    "        self.q_values[state][action] = q_value + self.lr*(reward + (self.discount_factor * max_next_q_value) - q_value)\n",
    "        self.n_values[state][action] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double Q learning**\n",
    "\n",
    "This algorithm mitigates the overestimation bias in Q-learning by maintaining two separate Q-value tables ($Q_a$ and $Q_b$).\n",
    "\n",
    "Update equation is:\n",
    "- update $Q_a$: $Q_a(s,a)=Q_a(s,a)+{\\alpha}[R+{\\gamma}\\underset{a'}{\\max}Q_b(s',a')−Q_a(s,a)]$\n",
    "- update $Q_b$: $Q_b(s,a)=Q_b(s,a)+{\\alpha}[R+{\\gamma}\\underset{a'}{\\max}Q_a(s',a')−Q_b(s,a)]$\n",
    "    - $Q_a(s,a), Q_b(s,a)$ is the Q-value for the current state-action pair from two separate tables.\n",
    "    - ${\\alpha}$ (learning rate) controls how much new information overrides old information.\n",
    "    - $R$ is the reward received after taking action $a$.\n",
    "    - ${\\gamma}$ (discount factor) determines the importance of future rewards.\n",
    "    - $\\underset{a'}{\\max}Q_a(s',a'), \\underset{a'}{\\max}Q_b(s',a')$ is the maximum Q-value in the next state from two separate table (off policy).\n",
    "    - At each step, either $Q_a$ or $Q_b$ is updated, chosen randomly with a probability of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update Q-values using Double Q-Learning.\n",
    "\n",
    "        This method applies the Double Q-Learning update rule to improve policy decisions by updating the Q-table.\n",
    "        \"\"\"\n",
    "        if np.random.random() < 0.5:\n",
    "            # Update Q_a\n",
    "            max_next_q_value = np.max(self.qb_values[next_state]) if next_state is not None else 0\n",
    "            q_value = self.qa_values[state][action]\n",
    "            self.qa_values[state][action] = q_value + self.lr * (reward + (self.discount_factor * max_next_q_value) - q_value)\n",
    "            self.na_values[state][action] += 1\n",
    "        else:\n",
    "            # Update Q_b\n",
    "            max_next_q_value = np.max(self.qa_values[next_state]) if next_state is not None else 0\n",
    "            q_value = self.qb_values[state][action]\n",
    "            self.qb_values[state][action] = q_value + self.lr * (reward + (self.discount_factor * max_next_q_value) - q_value)\n",
    "            self.nb_values[state][action] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Trainning & Playing to stabilize Cart-Pole Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result of playing cart-pole stabilize task\n",
    "\n",
    "<video controls src=\"img/2025-03-26 01-06-49.mp4\" title=\"Title\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluate Cart-Pole Agent performance.\n",
    "\n",
    "The goal of this evaluation is to determine the learning efficiency of each reinforcement learning algorithm (Monte-Carlo, SARSA, Q-learning, Double Q-learning) under varying hyperparameter conditions on the given cart-pole task.\n",
    "\n",
    "**Environment Configuration**\n",
    "- Reward Terms\n",
    "\n",
    "    | Index | Name        | Weight |\n",
    "    |-------|------------|--------|\n",
    "    | 0     | alive      | 1.0    |\n",
    "    | 1     | terminating | -2.0   |\n",
    "    | 2     | pole_pos   | -1.0   |\n",
    "\n",
    "- Termination Terms\n",
    "\n",
    "    | Index | Name               | Bound |\n",
    "    |-------|--------------------|------------------|\n",
    "    | 0     | time_out           |                  |\n",
    "    | 1     | cart_out_of_bounds | [-3, 3]          |\n",
    "    | 2     | pole_out_of_bounds | [-24, 24] deg    |\n",
    "\n",
    "- Event Terms (reset cart-pole after terminated)\n",
    "\n",
    "    | Index | Name                  | Bound |\n",
    "    |-------|-----------------------|----------------|\n",
    "    | 0     | reset_cart_position   |pos:[-1,1] vel:[-0.5,0.5]|\n",
    "    | 1     | reset_pole_position   |pos:[-24,24] vel:[-24,24]|\n",
    "\n",
    "- State Observation Terms\n",
    "\n",
    "    | Index | Name           | Shape  |\n",
    "    |-------|---------------|--------|\n",
    "    | 0     | joint_pos_rel | (2,)   |\n",
    "    | 1     | joint_vel_rel | (2,)   |\n",
    "\n",
    "- Action Terms\n",
    "\n",
    "    | Index | Name         | Dimension |\n",
    "    |-------|-------------|-----------|\n",
    "    | 0     | joint_effort | 1         |\n",
    "\n",
    "**Experiment**\n",
    "\n",
    "In this part is to analyze the agent's performance in terms of learning efficiency of each algorithm in different hyperparameters, So in this part will set to 3 experiment\n",
    "- minimize shape of observation state: To find the best 2 state from 4 state for visualize.\n",
    "- Adjust learning rate and discount factor: To see impact of learning rate and discount factor to each algorithm and find best learning rate and discount factor for this cart-pole task\n",
    "- Adjust size of action space and observation space: To see how size of action space and observation space impact to each algorithm which algorithm perform good in which size and find which algorithm perform best in this cart-pole task\n",
    "\n",
    "**Experiment Conditions**\n",
    "\n",
    "Fixed Hyperparameters Across Experiments:\n",
    "- action_range = [-15,15] (this one is perform best and fit to this task)\n",
    "- n_episodes = 5000\n",
    "- start_epsilon = 1.0\n",
    "- epsilon_decay = 0.9995\n",
    "- final_epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. minimize shape of observation state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment aims to simplify the visualization for the next experiments by reducing the number of observation states. Specifically, we compare a 4-state observation with different 2-state observation combinations to determine which performs best.\n",
    "\n",
    "**Perfroming results**\n",
    "- policy visualization\n",
    "\n",
    "<img src=\"img/image.png\" width=\"1000px\"/>\n",
    "<img src=\"img/image5.png\" width=\"1000px\"/>\n",
    "\n",
    "- reward visualization\n",
    "\n",
    "<img src=\"img/image2.png\" width=\"1000px\"/>\n",
    "\n",
    "From the graphs, we observe the following:\n",
    "- The best performing combination which highest returns and clear policy trends is pole position (pole-pos) and pole velocity (pole-vel).\n",
    "- However, when visualizing in cart-pole task performs, we notice that pole-pos with pole-vel focuses too much on the pole, causing oscillations and preventing a steady-state balance.\n",
    "- On the other hand, pole-pos with cart velocity (vel-cart), which ranks second in reward and policy trends, performs better in the cart-pole task. This combination avoids excessive focus on the pole and achieves better overall balance.\n",
    "\n",
    "**Conclusion**\n",
    "For future experiments, we will use pole-pos with vel-cart as the observation state. This combination provides better performance in the cart-pole task while preventing excessive focus on the pole, leading to more stable control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Adjust learning rate and discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is aims to evaluate how learning rate and discount factor affect each algorithm’s performance. And which learning rate and discount factor perform best in cart-pole task\n",
    "\n",
    "Parameters to vary:\n",
    "- Learning rates (${\\alpha}$): [0.1, 0.9]\n",
    "- Discount factors (${\\gamma}$): [0.7, 0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfroming results**\n",
    "- policy visualization\n",
    "\n",
    "    <img src=\"img/image3.png\" width=\"1000px\"/>\n",
    "    <img src=\"img/image6.png\" width=\"1000px\"/>\n",
    "\n",
    "- reward visualization\n",
    "    - Monte-Carlo\n",
    "\n",
    "    <img src=\"img/imageMC.png\" width=\"1000px\"/>\n",
    "\n",
    "    - SARSA\n",
    "\n",
    "    <img src=\"img/imageSARSA.png\" width=\"1000px\"/>\n",
    "\n",
    "    - Q-learning\n",
    "\n",
    "    <img src=\"img/imageQ.png\" width=\"1000px\"/>\n",
    "\n",
    "    - Double Q-learning\n",
    "\n",
    "    <img src=\"img/imageDQ.png\" width=\"1000px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount Factor**\n",
    "- Observation:\n",
    "    - Higher discount factor (${\\gamma}$ = 0.99) tends to perform worse in reward, meaning the episode ends sooner -> the agent fails earlier.\n",
    "    - max Q-value graph: At high discount, the policy tries to control both pole angle and cart velocity, aiming for a long-term stable balance. At low discount (${\\gamma}$ = 0.7), the policy only focuses on pole angle, regardless of cart velocity -> indicates preference for immediate correction.\n",
    "\n",
    "- Interpretation:\n",
    "    - Higher ${\\gamma}$ = more long-term reward -> more complex behavior, harder to learn without exploration.\n",
    "    - Lower ${\\gamma}$ = favors short-term reward -> quicker convergence and works better in this task where survival is short-term reactive (keep pole upright now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate**\n",
    "- Observation:\n",
    "    - Higher learning rate (${\\alpha}$ = 0.9) reaches better or similar reward in some config compare to lower ${\\alpha}$. In complex task learning faster than lower ${\\alpha}$.\n",
    "    - max Q-value graph becomes flatter, showing faster convergence, but sometimes less precise trends.\n",
    "\n",
    "- Interpretation:\n",
    "    - Higher ${\\alpha}$ = faster learning but each update changes the values a lot, so the learned policy might be less accurate or more random-looking.\n",
    "    - Lower ${\\alpha}$ = the agent updates more slowly, so the value it learns is more careful and detailed, leading to a smoother and more precise policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Adjust size of action space and observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is aims to evaluate how resolution of action space and observation space affect each algorithm’s performance.\n",
    "\n",
    "Parameters to vary:\n",
    "- num_of_action (discretize action to x value): [3, 11]\n",
    "- discretize_state_weight (discretize observation to x value): [[1,5,5,1], [1,21,21,1]] ([pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfroming results**\n",
    "- policy visualization\n",
    "\n",
    "    <img src=\"img/image4.png\" width=\"1000px\"/>\n",
    "    <img src=\"img/image7.png\" width=\"1000px\"/>\n",
    "\n",
    "- reward visualization\n",
    "    - Monte-Carlo\n",
    "\n",
    "    <img src=\"img/imageMC2.png\" width=\"1000px\"/>\n",
    "\n",
    "    - SARSA\n",
    "\n",
    "    <img src=\"img/imageSARSA2.png\" width=\"1000px\"/>\n",
    "\n",
    "    - Q-learning\n",
    "\n",
    "    <img src=\"img/imageQ2.png\" width=\"1000px\"/>\n",
    "\n",
    "    - Double Q-learning\n",
    "\n",
    "    <img src=\"img/imageDQ2.png\" width=\"1000px\"/>\n",
    "\n",
    "- Cart-pole task Perform\n",
    "    <video controls src=\"img/video2.mp4\" title=\"Title\"></video>\n",
    "\n",
    "    - Cart-pole task Perform graph\n",
    "    <img src=\"img/image9.png\" width=\"1000px\"/>\n",
    "    <img src=\"img/image8.png\" width=\"1000px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From graph we will see that\n",
    "| Action size | State size | Characteristic  |\n",
    "|-------------|------------|-----------------|\n",
    "| 3           | 5          |Coarsest setting; fastest to compute, lowest reward, more noise (except MC which is stable)|\n",
    "| 11          | 5          |Better than 3x5; less noisy reward, slightly better learning|\n",
    "| 3           | 21         |High state resolution improves reward obvously but takes more time|\n",
    "| 11          | 21         |Highest resolution; more precise but harder to train (SARSA & Double Q struggle)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Resolution**\n",
    "- Observation:\n",
    "    - Higher action resolution (more discrete actions like size 11) gives slightly better rewards than lower resolution (size 3), across all algorithms.\n",
    "    - With lower actions size, the Q-value trends become flat, especially in SARSA and Q-learning.\n",
    "\n",
    "- Interpretation:\n",
    "    - Higher action resolution allows the agent to choose actions that better match the environment’s needs, leading to more precise control and smoother policy.\n",
    "    - Lower action resolution forces the agent to choose from fewer options, making control less precise and leading to more noisy learning.\n",
    "    - In higher state resolution higher action will make agent more state-action pair make agent use more time. So changing action resolution is trade off between speed and precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State Resolution**\n",
    "- Observation:\n",
    "    - Higher state resolution leads to higher rewards but it use more time to compute.\n",
    "    - Higher state Resolution also leads to more complex Q-value graph.\n",
    "\n",
    "- Interpretation:\n",
    "    - High state resolution gives the agent more detailed information about the environment and it make agent can fine-tune actions more accurately and make high reward.\n",
    "    - Lower state resolution simplifies the state to small size make agent learn quicklier but causes the agent to group many situations into one state, make agent cannot fine tune to steady state and receive less reward.\n",
    "    - Higher state resolution make agent want more explore time to find all state and also use more overall time. So changing state resolution is also trade off between speed and precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compare each algorithm**\n",
    "- Sampling vs Bootstrap (MC vs Q-learning)\n",
    "    - MC is always leds **policy to update stable and smooth** in every situation because it update from real reward.\n",
    "    - Q-learning is update evrey step make agent more aggressive leading to highest reward making **policy more curved trend direction** but also make policy not smooth.\n",
    "- on-policy vs off-policy (SARSA vs Q-learning)\n",
    "    - SARSA is updates Q-values step-by-step using the next action chosen from epsilon greedy, If that action is random (due to exploration), **SARSA may learn from a non-optimal** or misleading reward making non-optimal policy.\n",
    "    - Q learning is have higher reward and also **can compute to more situation** because it calculate from actual exploit reward so it directly q-value to go to highest reward direction and with balance explore and exploit will make Q-learning better than SARSA.\n",
    "    - SARSA is better than Q-learning at **avoiding low-reward areas** because it updates using both random and greedy actions. If a random move leads to a risky zone, SARSA lowers the value of that state, encouraging the agent to avoid it.\n",
    "    - In contrast, Q-learning always uses the best next action, so nearby low-reward states might still appear valuable. This is reflected in the policy graphs, where SARSA shows lower Q-values near edges, learning to avoid danger more effectively.\n",
    "- Q-learning vs Double Q-learning\n",
    "    - Double Q-learning have less bias than Q-learning make agent **use more time** to update both table if it update just 1 table will make agent might more trust in value that update to 2 table because higher value.\n",
    "    - In this task Q-learning is better because it balance explore and exploit and **task didn't complex** so bias is lead agent to correct direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm        | Pros                                                                 | Cons                                                                      |\n",
    "|------------------|----------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| **Monte Carlo**  | - Stable and smooth policy updates compare to bootstrap              | - Slower learning than Q-learning and SARSA (updates only at episode end) |\n",
    "|                  | - Accurate value estimates (uses real total reward)                  |                                                                           |\n",
    "| **SARSA**        | - Learns to avoid low-reward or risky states (better than Q-learning)| - Slower to reach high reward than Q-learning                             |\n",
    "|                  |                                                                      | - Can learn from suboptimal actions (random action from ε-greedy) and make policy non-optimal |\n",
    "| **Q-learning**   | - Faster learning than SARSA and MC (updates every step with greedy next action)       | - Policy surface less smooth than MC                                   |\n",
    "|                  | - High reward performance in simple environments (bias helps when task is not complex) | - May ignore nearby risky areas (focuses only on best-case outcome)    |\n",
    "|                  | - More direct policy trend (clear direction in value graph)                            |                                                                        |\n",
    "| **Double Q**     | - Lower bias than Q-learning                                                           | - Slower than Q-learning (requires more steps to update both Q-tables) |\n",
    "|                  | - More stable in complex or noisy environments                                         | - Needs more exploration to fully learn both Q-tables                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion the best algorithm is MC because MC have best perform in cart-pole task from it stable update policy and smooth of policy that make every episode MC can make pole stable in equally time. And task is small task so it doesn't store lot data or use very long time to update Monte-Carlo compare to others.\n",
    "\n",
    "For Q-learning that have better reward it also perform good in cart-pole task but there are some unstable in some episode which is impact from aggressive learning that make policy not smooth and it better than SARSA and Double Q-learning because task is have less complex so bias from Q-learning can make agent learn faster and have highest reward and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
